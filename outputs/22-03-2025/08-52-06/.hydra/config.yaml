model:
  name: meta-llama/Meta-Llama-3-8B-Instruct
  max_new_tokens: 12
  temperature: 0.1
  to_continue: false
  mode: eval
setting:
  name: Baseline
  interpretability: true
data:
  path: ${oc.env:HOME}/tasks_1-20_v1-2/en-valid/
  splits:
    train: false
    valid: true
    test: false
  task_ids: null
  samples_per_task: 5
  to_enumerate:
    context: true
    question: false
  wrapper:
    context: '*TASK*

      Here are the context sentences:

      {context}

      '
    question: 'Now, answer the following question:

      {question}

      '
    reasoning: ''
    answer: ''
init_prompt:
  paths:
  - ${oc.env:HOME}/research-project/inference/prompts/init_prompt_direct_answer.txt
  examples:
    add: false
    enumerated: true
    handpicked: true
    not_mentioned: true
    number: 1
    wrapper: '*EXAMPLE{number}*

      {example}

      *END OF EXAMPLE{number}*

      '
logging:
  print_to_file: true
results:
  save_heatmaps: true
  headers:
  - id
  - task_id
  - sample_id
  - part_id
  - task
  - golden_answer
  - silver_reasoning
  - model_answer_after
  - correct_after
  - model_reasoning_before
  - model_reasoning_after
  - silver_reasoning
  - model_output_after
  - exact_match_accuracy_after
  - soft_match_accuracy_after
  - not_mentioned_after
  - verbs_after
  - there_after
  - pronouns_after
