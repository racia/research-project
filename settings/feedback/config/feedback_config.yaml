teacher:
  name: "meta-llama/Meta-Llama-3-70B-Instruct"
  max_new_tokens: 150
  temperature: 0.1
  to_continue: False
  mode: "eval"
  interpretability:
    use: False

student:
  name: "meta-llama/Meta-Llama-3-8B-Instruct"
  max_new_tokens: 200
  temperature: 0.1
  to_continue: False
  mode: "eval"
  interpretability:
    use: True
    aggregate: True

setting:
  name: 'Feedback'

data:
  path: "${oc.env:HOME}/tasks_1-20_v1-2/en-valid/"
  baseline_results: "/pfs/work9/workspace/scratch/hd_nc326-research-project/baseline/valid/basic_baseline/baseline_prompt/valid_baseline_prompt_results.csv"
  # Order: train, valid, test
  splits:
    train: False
    valid: True
    test: False
  # to get all tasks use ~ (will be converted to None)
  # to get specific tasks use:
  # task_ids: [1, 2, ...]
  task_ids: ~
  samples_per_task: 5
  # if to add line numbers to the sentences in the prompt
  # Order: context, question
  to_enumerate:
    context: True
    question: False
  # if to wrap the part prompt with the task description
  wrapper:
    context: |
      *TASK*
      Here are the context sentences:
      {context}
    question: |
      Now, answer the following question:
      {question}
    reasoning: ""
    answer: ""

init_prompt:
  paths:
    - "${oc.env:HOME}/research-project/inference/prompts/init_prompt_reasoning.txt"
  examples:
    add: True
    enumerated: True
    handpicked: True
    not_mentioned: True
    number: 1
    wrapper: |
      *EXAMPLE*
      {example}
      *END OF EXAMPLE*

feedback_prompt:
  paths:
    - "${oc.env:HOME}/research-project/inference/prompts/feedback_prompt.txt"
  history: |
    Your student was given the following task:
    """
    {chat_history}
    """
  wrapper: |
    The student's response was:
    {student_output}

refine_prompt:
  # name of the prompt files serves as a basis for logging and result files
  paths:
    - "${oc.env:HOME}/research-project/inference/prompts/refine_prompt.txt"

logging:
  # if to print the results to a log file, otherwise will print to console
  print_to_file: True

results:
  headers:
    - id_
    - task_id
    - sample_id
    - part_id
    - task
    - answer_lies_in_self
    - golden_answer
    - silver_reasoning
    - model_answer_after
    - answer_correct_after
    - reasoning_correct_after
    - model_reasoning_after
    - model_output_after
    - exact_match_accuracy_after
    - soft_match_accuracy_after
    - max_supp_attn_after
    - attn_on_target_after
    - there_after
    - verbs_after
    - pronouns_after
    - not_mentioned_after
    - context_sents_hall_after
    - model_answer_before
    - answer_correct_before
    - reasoning_correct_before
    - model_reasoning_before
    - model_output_before
    - exact_match_accuracy_before
    - soft_match_accuracy_before
    - max_supp_attn_before
    - attn_on_target_before
    - there_before
    - verbs_before
    - pronouns_before
    - not_mentioned_before
    - iterations
    - context_sents_hall_before
    - max_supp_attn_corr_before
    - attn_on_target_corr_before
    - sample_part_lengths_corr_before


  hydra:
    run:
      # the run directory with logs, plots, and csv results will be created here
      # use more folders if needed for distinguishing between different experiments
      dir: "/pfs/work9/workspace/scratch/hd_nc326-research-project/feedback/${now:%d-%m-%Y}/${now:%H-%M-%S}"
